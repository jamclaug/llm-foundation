# Phase 2: Architecture

Transformer building blocks - encoder-decoder patterns and GPT model architecture.

## Learning Objectives
- Build transformer encoder and decoder blocks
- Implement GPT-style decoder-only architecture
- Understand positional encodings and layer normalization
- Create training utilities for model development

## Directory Structure

### `notebooks/`
Interactive exploration (to be created):
- Transformer block development
- GPT architecture exploration

### `src/`
Implementation files (placeholders):
- `encoder_decoder.py` - Transformer encoder/decoder blocks
- `gpt_model.py` - GPT-style decoder-only model
- `training_utils.py` - Training loops, checkpointing, logging

### `tests/`
- Validation scripts for transformer components

## Implementation Notes
- Build on phase1 attention mechanisms
- Use PyTorch for transformer blocks
- Focus on understanding architecture before optimization
