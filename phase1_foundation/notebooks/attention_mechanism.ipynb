{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833f3f6",
   "metadata": {},
   "source": [
    "## 1. Basic Self-Attention\n",
    "\n",
    "Self-attention computes attention scores between all positions in a sequence.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa699ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch_size, seq_len, d_k)\n",
    "        K: Key tensor (batch_size, seq_len, d_k)\n",
    "        V: Value tensor (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (useful for causal/decoder attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply attention weights to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551338fe",
   "metadata": {},
   "source": [
    "### Test Basic Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ef7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Random embeddings for a sequence of 4 tokens\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# For self-attention, Q, K, V are all projections of the same input\n",
    "Q = K = V = x\n",
    "\n",
    "# Compute attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (how much each position attends to others):\")\n",
    "print(attn_weights[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53972357",
   "metadata": {},
   "source": [
    "## 2. Visualize Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4076de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention_weights, tokens=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weight matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Convert to numpy and squeeze batch dimension\n",
    "    weights = attention_weights.squeeze(0).detach().numpy()\n",
    "    \n",
    "    plt.imshow(weights, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.title('Attention Weights Heatmap')\n",
    "    \n",
    "    if tokens:\n",
    "        plt.xticks(range(len(tokens)), tokens)\n",
    "        plt.yticks(range(len(tokens)), tokens)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the attention pattern\n",
    "plot_attention_weights(attn_weights, tokens=['Token1', 'Token2', 'Token3', 'Token4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f64f14",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention\n",
    "\n",
    "Multi-head attention runs multiple attention operations in parallel, allowing the model to attend to different aspects of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-Head Attention module.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Dimension of the model (embedding size)\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k).\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads back into (batch_size, seq_len, d_model).\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Compute attention for each head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Combine heads\n",
    "        output = self.combine_heads(output)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dec5a",
   "metadata": {},
   "source": [
    "### Test Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24776964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# For self-attention, Q=K=V\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape (per head): {attn_weights.shape}\")\n",
    "print(f\"Number of attention heads: {num_heads}\")\n",
    "print(f\"Dimension per head (d_k): {d_model // num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3a031",
   "metadata": {},
   "source": [
    "## 4. Compare with PyTorch Built-in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf432a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's built-in multi-head attention\n",
    "pytorch_mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output_pytorch, attn_weights_pytorch = pytorch_mha(x, x, x)\n",
    "\n",
    "print(f\"PyTorch MHA Output shape: {output_pytorch.shape}\")\n",
    "print(f\"PyTorch MHA Attention weights shape: {attn_weights_pytorch.shape}\")\n",
    "print(\"\\nBoth implementations produce the same output shapes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b8229",
   "metadata": {},
   "source": [
    "## 5. Causal (Masked) Attention\n",
    "\n",
    "In decoder-only models like GPT, we need causal masking so each position can only attend to earlier positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a8f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask to prevent attending to future positions.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return ~mask  # Invert: 1 means attend, 0 means mask\n",
    "\n",
    "# Create and visualize causal mask\n",
    "seq_len = 6\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(causal_mask.numpy(), cmap='gray', aspect='auto')\n",
    "plt.title('Causal Attention Mask\\n(White = Can Attend, Black = Masked)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Causal mask (1 = attend, 0 = mask):\")\n",
    "print(causal_mask.int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6855e95",
   "metadata": {},
   "source": [
    "## 6. Test Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef31cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = create_causal_mask(seq_len).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Compute attention with mask\n",
    "output, attn_weights = scaled_dot_product_attention(x, x, x, mask=mask)\n",
    "\n",
    "print(\"Attention weights with causal masking:\")\n",
    "print(attn_weights[0].detach().numpy())\n",
    "print(\"\\nNotice: Each row sums to 1, but only attends to current and previous positions!\")\n",
    "\n",
    "# Visualize\n",
    "plot_attention_weights(attn_weights, tokens=[f'T{i}' for i in range(seq_len)])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
