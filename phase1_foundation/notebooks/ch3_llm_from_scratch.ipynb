{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0039ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts (x^3)\n",
    "     [0.22, 0.58, 0.33], # with (x^4)\n",
    "     [0.77, 0.25, 0.10], # one (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8a906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # Query vector (x^2)\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # To store attention scores\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # Dot product\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42069ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)  # Should match attn_scores_2[0]\n",
    "print(torch.dot(inputs[0], query))  # Direct dot product for verification  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678e8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc946f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Softmax Weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Naive Softmax Weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f380584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Softmax Weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"PyTorch Softmax Weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ac75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(\"Context Vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e86864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty((inputs.shape[0], inputs.shape[0]))\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(\"Attention Scores Matrix:\\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69712468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores Matrix using @ operator:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(\"Attention Scores Matrix using @ operator:\\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "641d5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449dcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284a6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "torch.Size([6, 3])\n",
      "All Context Vectors:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(attn_weights.shape)\n",
    "print(inputs.shape)\n",
    "print(\"All Context Vectors:\\n\", all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "348d70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector: tensor([0.4306, 1.4551])\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 \n",
    "\n",
    "# Initialize the three weight matrices\n",
    "torch.manual_seed(123)  # For reproducibility\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# We set requires_grad=False for simplicity in this example, if we were training the model, we would set it to True\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(\"Query Vector:\", query_2)\n",
    "\n",
    "# In the weight matrices W, the term \"weight\" refers to the learnable parameters that transform the input vectors into query, key, and value vectors.\n",
    "# These are not the attention weights computed later in the attention mechanism.\n",
    "\n",
    "# Weight parameters are the fundamental, learned coefficients that define how input data is processed within the model.\n",
    "# Attention weights, on the other hand, are dynamic coefficients computed during the attention mechanism that determine \n",
    "# the relevance of different input elements to each other based on their content. \n",
    "# They are derived from the interactions between the query and key vectors and are used to weight the value vectors\n",
    "\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16279586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Score for x^2 with x^2: tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(\"Attention Score for x^2 with x^2:\", attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac731ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores for x^2 with all keys: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(\"Attention Scores for x^2 with all keys:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e43d34bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Attention Weights: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# To go from attention scores to attention weights, we apply the softmax function\n",
    "d_k = keys.shape[-1] # Dimension of the key vectors\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=0) # Scale by sqrt(d_k)\n",
    "print(\"Scaled Attention Weights:\", attn_weights_2)\n",
    "\n",
    "# The reason for normalization by the embedding dimension's square root (sqrt(d_k)) is to prevent the dot products from becoming too large in magnitude.\n",
    "# When the dimensionality of the key and query vectors is high, the dot products can grow\n",
    "# very large, which can push the softmax function into regions where it has extremely small gradients.\n",
    "# This can make the model harder to train effectively. By scaling the dot products down by sqrt(d_k),\n",
    "# we help keep the values in a range that is more suitable for the softmax function, ensuring better gradient flow during training.\n",
    "# This technique was introduced in the \"Attention is All You Need\" paper by Vaswani et al.\n",
    "# This is also called scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18eb909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# The final step is to compute the context vector as a weighted sum of the value vectors\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context Vector:\", context_vec_2)\n",
    "\n",
    "# This is the context vector z^2 for the input x^2, computed using the attention mechanism with learned query, key, and value projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0312df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why query, key and value?\n",
    "# The concepts of query, key, and value in the attention mechanism are inspired by information retrieval systems.\n",
    "# In such systems, a \"query\" is used to search for relevant \"keys\" in a database, and the associated \"values\" are the information retrieved based on that search.\n",
    "# In the context of neural networks and attention mechanisms, this analogy helps to conceptualize how different parts of the input data interact with each other.\n",
    "# The query vector represents the current focus of attention (what we are looking for),  \n",
    "# the key vectors represent the features of the input data that can be matched against the query (what we have),\n",
    "# and the value vectors represent the actual information to be aggregated based on the attention scores (what we retrieve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ec93b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Self attention class\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @keys.T # omega shape: (seq_len, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "ea_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(ea_v1(inputs))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d65ede6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T # omega shape: (seq_len, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fcab4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3407de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.Linear in SelfAttention_v2 uses a different weight initialization strategy compared to manually initializing weights as done in SelfAttention_v1.\n",
    "# nn.Linear typically uses Kaiming Uniform or Xavier Uniform initialization methods, which are designed to maintain the variance of activations through layers, promoting better convergence during training.\n",
    "# To prove that these produce similar results, we can manually set the weights of SelfAttention_v2 to match those of SelfAttention_v1 after initialization.\n",
    "# Need to transpose the weights because of the way nn.Linear stores weights\n",
    "# in transposed form\n",
    "sa_v2.W_query.weight.data = ea_v1.W_query.data.T.clone()\n",
    "sa_v2.W_key.weight.data = ea_v1.W_key.data.T.clone()\n",
    "sa_v2.W_value.weight.data = ea_v1.W_value.data.T.clone()\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53e3b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae993d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0] # Number of tokens in the input sequence\n",
    "mask_simple = torch.tril(torch.ones((context_length, context_length)))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9d9801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention Weights:\n",
      " tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1500, 0.2264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1503, 0.2256, 0.2192, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.0000, 0.0000],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.0000],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(\"Masked Attention Weights:\\n\", masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ab34aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551],\n",
      "        [0.3764],\n",
      "        [0.5952],\n",
      "        [0.7024],\n",
      "        [0.8248],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "Renormalized Masked Attention Weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now renormalize the masked weights so that they sum to 1\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(\"Renormalized Masked Attention Weights:\\n\", masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5037c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention Scores:\n",
      " tensor([[0.9231,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2705, 1.8524,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2544, 1.8284, 1.7877,   -inf,   -inf,   -inf],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925,   -inf,   -inf],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707,   -inf],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Masked Attention Weights with -inf:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask == 1, -torch.inf)\n",
    "print(\"Masked Attention Scores:\\n\", masked)\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Masked Attention Weights with -inf:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb921ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7582, 0.7366, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5587, 0.4206, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3068, 0.0000],\n",
      "        [0.3115, 0.4183, 0.0000, 0.2839, 0.2178, 0.3588]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Add dropout to prevent overfitting\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61f7fb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)  # Shape: (2, 6, 3)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfe99226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal (Masked) Self-Attention module for decoder-only models like GPT.\n",
    "    Prevents each position from attending to future positions in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_in: Input embedding dimension\n",
    "            d_out: Output dimension (attention head dimension)\n",
    "            context_length: Maximum sequence length (for creating causal mask)\n",
    "            dropout: Dropout probability for attention weights\n",
    "            qkv_bias: Whether to include bias in Q, K, V projections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Learnable linear projections for Query, Key, Value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create a causal mask (upper triangular matrix of 1s)\n",
    "        # register_buffer: not a trainable parameter, but moves with model (e.g., to GPU)\n",
    "        # torch.triu creates upper triangle: 1s above diagonal, 0s on/below diagonal\n",
    "        # This mask blocks attention to future positions\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_tokens, d_in)\n",
    "        \n",
    "        Returns:\n",
    "            context_vec: Output tensor of shape (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # Project input into Query, Key, Value spaces\n",
    "        keys = self.W_key(x)       # Shape: (batch, num_tokens, d_out)\n",
    "        queries = self.W_query(x)  # Shape: (batch, num_tokens, d_out)\n",
    "        values = self.W_value(x)   # Shape: (batch, num_tokens, d_out)\n",
    "\n",
    "        # Compute attention scores: Q @ K^T\n",
    "        # transpose(1, 2) swaps token and d_out dimensions, keeping batch dimension first\n",
    "        # Result: how much each token (row) should attend to every other token (column)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # Shape: (batch, num_tokens, num_tokens)\n",
    "        \n",
    "        # Apply causal mask: set future positions to -inf\n",
    "        # [:num_tokens, :num_tokens] allows variable sequence lengths\n",
    "        # After softmax, -inf becomes 0 (no attention to future)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        \n",
    "        # Scaled dot-product attention: divide by sqrt(d_k) to prevent large values\n",
    "        # Then apply softmax to get probabilities that sum to 1 per row\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights (randomly zero some connections during training)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Compute context vectors as weighted sum of values\n",
    "        # Each output token is a combination of value vectors weighted by attention\n",
    "        context_vec = attn_weights @ values  # Shape: (batch, num_tokens, d_out)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69b856fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test the CausalAttention module\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]  # Get sequence length from batch\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bf50cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(\n",
    "                d_in, d_out, context_length, dropout, qkv_bias                \n",
    "            )\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac9c75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 4])\n",
      "Context Vectors from Multi-Head Attention:\n",
      " tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]  # Get sequence length from batch\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "print(\"Context Vectors from Multi-Head Attention:\\n\", context_vecs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
